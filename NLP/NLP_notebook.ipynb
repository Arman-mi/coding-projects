{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TweetID       entity sentiment  \\\n",
      "0     2401  Borderlands  Positive   \n",
      "1     2401  Borderlands  Positive   \n",
      "2     2401  Borderlands  Positive   \n",
      "3     2401  Borderlands  Positive   \n",
      "4     2401  Borderlands  Positive   \n",
      "\n",
      "                                             message  \n",
      "0  im getting on borderlands and i will murder yo...  \n",
      "1  I am coming to the borders and I will kill you...  \n",
      "2  im getting on borderlands and i will kill you ...  \n",
      "3  im coming on borderlands and i will murder you...  \n",
      "4  im getting on borderlands 2 and i will murder ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\arman\\python\\projects\\NLP\\Data\\twitter_training.csv', encoding='ISO-8859-1', header=None)\n",
    "data.columns=['TweetID','entity','sentiment','message']\n",
    "\n",
    "print(data.head())\n",
    "messages=data[\"message\"]\n",
    "messages = messages.fillna('')\n",
    "messages = messages.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TweetID       entity sentiment  \\\n",
      "0     2401  Borderlands  Positive   \n",
      "1     2401  Borderlands  Positive   \n",
      "2     2401  Borderlands  Positive   \n",
      "3     2401  Borderlands  Positive   \n",
      "4     2401  Borderlands  Positive   \n",
      "\n",
      "                                             message  message_length  \n",
      "0  im getting on borderlands and i will murder yo...              53  \n",
      "1  I am coming to the borders and I will kill you...              51  \n",
      "2  im getting on borderlands and i will kill you ...              50  \n",
      "3  im coming on borderlands and i will murder you...              51  \n",
      "4  im getting on borderlands 2 and i will murder ...              57  \n",
      "count     74682\n",
      "unique    69492\n",
      "top            \n",
      "freq        686\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(data.info)\n",
    "# print(data['sentiment'].value_counts())\n",
    "# print(messages.isnull().sum())\n",
    "# print(messages.dtype)\n",
    "data['message_length'] = messages.apply(len) \n",
    "print(data.head())\n",
    "print(messages.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      im getting borderland murder\n",
      "1                coming border kill\n",
      "2        im getting borderland kill\n",
      "3       im coming borderland murder\n",
      "4    im getting borderland 2 murder\n",
      "Name: cleaned_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+|[^A-Za-z0-9\\s]+', '', tweet)\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    words = [word for word in tweet.split() if word not in stop_words]\n",
    "    # Lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing function to the tweet column\n",
    "data['cleaned_tweet'] = messages.apply(preprocess_tweet)\n",
    "\n",
    "# Check the cleaned tweets\n",
    "print(data['cleaned_tweet'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(r'C:\\Users\\arman\\python\\projects\\NLP\\Data\\twitter_validation.csv', encoding='ISO-8859-1', header = None)\n",
    "validation_data.columns= ['TweetID','entity','sentiment','message']\n",
    "\n",
    "#validation_data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_data['cleaned_tweet'] = validation_data['message'].apply(preprocess_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (74682, 5000)\n",
      "Validation data shape: (1000, 5000)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "this is the part that we build a model and convert the texts into features.\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "X_train = vectorizer.fit_transform(data['cleaned_tweet'])\n",
    "X_val = vectorizer.transform(validation_data['cleaned_tweet'])\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "print(f'Validation data shape: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(X_train, data['sentiment'])\n",
    "y_pred = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 80.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.70      0.74       172\n",
      "     Neutral       0.77      0.88      0.82       266\n",
      "    Positive       0.86      0.74      0.79       285\n",
      "  Irrelevant       0.79      0.84      0.82       277\n",
      "\n",
      "    accuracy                           0.80      1000\n",
      "   macro avg       0.80      0.79      0.79      1000\n",
      "weighted avg       0.80      0.80      0.80      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(validation_data['sentiment'], y_pred)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment'], y_pred, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'max_iter': [1000, 2000, 3000]}\n",
    "# grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1_weighted')\n",
    "# grid.fit(X_train, data['sentiment'])\n",
    "# print(\"Best parameters:\", grid.best_params_)\n",
    "\n",
    "# # Use the best estimator for predictions\n",
    "# best_model = grid.best_estimator_\n",
    "# y_pred_best = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'log_reg__C': 0.01, 'log_reg__max_iter': 5000, 'log_reg__solver': 'saga'}\n",
      "Validation Accuracy: 85.20%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.87      0.84       172\n",
      "     Neutral       0.82      0.90      0.86       266\n",
      "    Positive       0.92      0.79      0.85       285\n",
      "  Irrelevant       0.85      0.87      0.86       277\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.85      0.85      0.85      1000\n",
      "weighted avg       0.86      0.85      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a pipeline that scales the data and then applies Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=False)),  # with_mean=False because TF-IDF produces sparse data\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Set up the hyperparameter grid\n",
    "param_grid = {\n",
    "    'log_reg__C': [0.01, 0.1, 1, 10, 100],        # Regularization strength\n",
    "    'log_reg__max_iter': [5000],      # Number of iterations for convergence\n",
    "    'log_reg__solver': [ 'saga']          # Try alternative solvers\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline, parameter grid, and cross-validation settings\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted')\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid.fit(X_train, data['sentiment'])\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best parameters found: \", grid.best_params_)\n",
    "\n",
    "# Use the best model from grid search for predictions\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Evaluate the performance of the best model on validation data\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(validation_data['sentiment'], y_pred)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment'], y_pred, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Accuracy: 45.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.03      0.06       172\n",
      "     Neutral       0.37      0.94      0.53       266\n",
      "    Positive       0.88      0.16      0.27       285\n",
      "  Irrelevant       0.58      0.57      0.58       277\n",
      "\n",
      "    accuracy                           0.46      1000\n",
      "   macro avg       0.71      0.42      0.36      1000\n",
      "weighted avg       0.68      0.46      0.39      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, data['sentiment'])\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(validation_data['sentiment'], y_pred_rf)\n",
    "print(f'Random Forest Validation Accuracy: {accuracy_rf * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment'], y_pred_rf, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.2-py3-none-win_amd64.whl (124.9 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.26.4)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.2-py3-none-win_amd64.whl (124.9 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.14.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for sentiment labels\n",
    "label_mapping = {\n",
    "    'Negative': 0,\n",
    "    'Neutral': 1,\n",
    "    'Positive': 2,\n",
    "    'Irrelevant': 3\n",
    "}\n",
    "\n",
    "# Apply the mapping to both training and validation datasets\n",
    "data['sentiment_numeric'] = data['sentiment'].map(label_mapping)\n",
    "validation_data['sentiment_numeric'] = validation_data['sentiment'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arman\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [06:22:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Accuracy: 83.70%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.92      0.83       266\n",
      "     Neutral       0.90      0.77      0.83       285\n",
      "    Positive       0.83      0.88      0.85       277\n",
      "  Irrelevant       0.94      0.76      0.84       172\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.86      0.83      0.84      1000\n",
      "weighted avg       0.85      0.84      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=100, max_depth=10, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Train the model on the training data using numeric labels\n",
    "xgb_model.fit(X_train, data['sentiment_numeric'])\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(validation_data['sentiment_numeric'], y_pred_xgb)\n",
    "print(f'XGBoost Validation Accuracy: {accuracy_xgb * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment_numeric'], y_pred_xgb, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arman\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [06:54:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     12\u001b[0m     XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     13\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment_numeric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with a limited number of iterations\n",
    "random_search = RandomizedSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,  # Limits the number of parameter combinations to try\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, data['sentiment_numeric'])\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Requirement already satisfied: xgboost in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\arman\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.14.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost --extra-index-url https://pypi.nvidia.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m      6\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,         \u001b[38;5;66;03m# Number of trees, you can adjust for experimentation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,             \u001b[38;5;66;03m# Maximum depth of each tree\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment_numeric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment_numeric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation data for early stopping\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Stops if no improvement after 10 rounds\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Prints progress and performance metrics during training\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation set\u001b[39;00m\n\u001b[0;32m     24\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the XGBoost model with GPU support\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,         # Number of trees, you can adjust for experimentation\n",
    "    max_depth=10,             # Maximum depth of each tree\n",
    "    learning_rate=0.1,        # Step size\n",
    "    tree_method='gpu_hist',   # Enables GPU support\n",
    "    use_label_encoder=False,  # Avoids unnecessary warnings in recent XGBoost versions\n",
    "    eval_metric='mlogloss',   # Multi-class logarithmic loss for evaluation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "xgb_model.fit(\n",
    "    X_train, data['sentiment_numeric'],\n",
    "    eval_set=[(X_val, validation_data['sentiment_numeric'])],  # Validation data for early stopping\n",
    "    early_stopping_rounds=10,  # Stops if no improvement after 10 rounds\n",
    "    verbose=True  # Prints progress and performance metrics during training\n",
    ")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(validation_data['sentiment_numeric'], y_pred_xgb)\n",
    "print(f'XGBoost Validation Accuracy: {accuracy_xgb * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment_numeric'], y_pred_xgb, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arman\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [07:02:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\arman\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [07:02:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\context.cc:43: No visible GPU is found, setting device to CPU.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\arman\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [07:02:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\context.cc:196: XGBoost is not compiled with CUDA support.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[07:02:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:861: Exception in gpu_hist: [07:02:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:867: Check failed: ctx_->Ordinal() >= 0 (-1 vs. 0) : Must have at least one device\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Multi-class classification\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m,                \u001b[38;5;66;03m# Number of classes in the target variable\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m      \u001b[38;5;66;03m# Evaluation metric\u001b[39;00m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Maximum number of boosting rounds\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation set for evaluation\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Stop if no improvement after 10 rounds\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Print progress during training\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation data\u001b[39;00m\n\u001b[0;32m     27\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m bst\u001b[38;5;241m.\u001b[39mpredict(dval)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:2100\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2100\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [07:02:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:861: Exception in gpu_hist: [07:02:11] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:867: Check failed: ctx_->Ordinal() >= 0 (-1 vs. 0) : Must have at least one device\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "dtrain = xgb.DMatrix(X_train, label=data['sentiment_numeric'])\n",
    "dval = xgb.DMatrix(X_val, label=validation_data['sentiment_numeric'])\n",
    "\n",
    "# Set up XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Multi-class classification\n",
    "    'num_class': 4,                # Number of classes in the target variable\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'tree_method': 'gpu_hist',     # Use GPU for training\n",
    "    'eval_metric': 'mlogloss'      # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "bst = xgb.train(\n",
    "    params, \n",
    "    dtrain, \n",
    "    num_boost_round=300,           # Maximum number of boosting rounds\n",
    "    evals=[(dval, 'validation')],  # Validation set for evaluation\n",
    "    early_stopping_rounds=10,      # Stop if no improvement after 10 rounds\n",
    "    verbose_eval=True              # Print progress during training\n",
    ")\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_xgb = bst.predict(dval)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(validation_data['sentiment_numeric'], y_pred_xgb)\n",
    "print(f'XGBoost Validation Accuracy: {accuracy_xgb * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment_numeric'], y_pred_xgb, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mlogloss:1.36165\n",
      "[1]\tvalidation-mlogloss:1.33937\n",
      "[2]\tvalidation-mlogloss:1.32007\n",
      "[3]\tvalidation-mlogloss:1.30362\n",
      "[4]\tvalidation-mlogloss:1.28841\n",
      "[5]\tvalidation-mlogloss:1.27422\n",
      "[6]\tvalidation-mlogloss:1.26240\n",
      "[7]\tvalidation-mlogloss:1.25083\n",
      "[8]\tvalidation-mlogloss:1.24018\n",
      "[9]\tvalidation-mlogloss:1.23025\n",
      "[10]\tvalidation-mlogloss:1.22223\n",
      "[11]\tvalidation-mlogloss:1.21148\n",
      "[12]\tvalidation-mlogloss:1.20220\n",
      "[13]\tvalidation-mlogloss:1.19444\n",
      "[14]\tvalidation-mlogloss:1.18688\n",
      "[15]\tvalidation-mlogloss:1.17877\n",
      "[16]\tvalidation-mlogloss:1.17029\n",
      "[17]\tvalidation-mlogloss:1.16369\n",
      "[18]\tvalidation-mlogloss:1.15744\n",
      "[19]\tvalidation-mlogloss:1.15101\n",
      "[20]\tvalidation-mlogloss:1.14490\n",
      "[21]\tvalidation-mlogloss:1.13904\n",
      "[22]\tvalidation-mlogloss:1.13128\n",
      "[23]\tvalidation-mlogloss:1.12506\n",
      "[24]\tvalidation-mlogloss:1.11966\n",
      "[25]\tvalidation-mlogloss:1.11433\n",
      "[26]\tvalidation-mlogloss:1.10841\n",
      "[27]\tvalidation-mlogloss:1.10264\n",
      "[28]\tvalidation-mlogloss:1.09825\n",
      "[29]\tvalidation-mlogloss:1.09294\n",
      "[30]\tvalidation-mlogloss:1.08762\n",
      "[31]\tvalidation-mlogloss:1.08274\n",
      "[32]\tvalidation-mlogloss:1.07831\n",
      "[33]\tvalidation-mlogloss:1.07262\n",
      "[34]\tvalidation-mlogloss:1.06782\n",
      "[35]\tvalidation-mlogloss:1.06269\n",
      "[36]\tvalidation-mlogloss:1.05897\n",
      "[37]\tvalidation-mlogloss:1.05341\n",
      "[38]\tvalidation-mlogloss:1.04925\n",
      "[39]\tvalidation-mlogloss:1.04582\n",
      "[40]\tvalidation-mlogloss:1.04160\n",
      "[41]\tvalidation-mlogloss:1.03733\n",
      "[42]\tvalidation-mlogloss:1.03322\n",
      "[43]\tvalidation-mlogloss:1.02987\n",
      "[44]\tvalidation-mlogloss:1.02542\n",
      "[45]\tvalidation-mlogloss:1.02125\n",
      "[46]\tvalidation-mlogloss:1.01743\n",
      "[47]\tvalidation-mlogloss:1.01399\n",
      "[48]\tvalidation-mlogloss:1.01088\n",
      "[49]\tvalidation-mlogloss:1.00814\n",
      "[50]\tvalidation-mlogloss:1.00461\n",
      "[51]\tvalidation-mlogloss:1.00072\n",
      "[52]\tvalidation-mlogloss:0.99684\n",
      "[53]\tvalidation-mlogloss:0.99316\n",
      "[54]\tvalidation-mlogloss:0.99063\n",
      "[55]\tvalidation-mlogloss:0.98704\n",
      "[56]\tvalidation-mlogloss:0.98459\n",
      "[57]\tvalidation-mlogloss:0.98164\n",
      "[58]\tvalidation-mlogloss:0.97872\n",
      "[59]\tvalidation-mlogloss:0.97493\n",
      "[60]\tvalidation-mlogloss:0.97213\n",
      "[61]\tvalidation-mlogloss:0.96918\n",
      "[62]\tvalidation-mlogloss:0.96590\n",
      "[63]\tvalidation-mlogloss:0.96222\n",
      "[64]\tvalidation-mlogloss:0.95968\n",
      "[65]\tvalidation-mlogloss:0.95684\n",
      "[66]\tvalidation-mlogloss:0.95415\n",
      "[67]\tvalidation-mlogloss:0.95181\n",
      "[68]\tvalidation-mlogloss:0.94962\n",
      "[69]\tvalidation-mlogloss:0.94745\n",
      "[70]\tvalidation-mlogloss:0.94486\n",
      "[71]\tvalidation-mlogloss:0.94215\n",
      "[72]\tvalidation-mlogloss:0.94005\n",
      "[73]\tvalidation-mlogloss:0.93765\n",
      "[74]\tvalidation-mlogloss:0.93568\n",
      "[75]\tvalidation-mlogloss:0.93329\n",
      "[76]\tvalidation-mlogloss:0.93151\n",
      "[77]\tvalidation-mlogloss:0.92972\n",
      "[78]\tvalidation-mlogloss:0.92745\n",
      "[79]\tvalidation-mlogloss:0.92496\n",
      "[80]\tvalidation-mlogloss:0.92296\n",
      "[81]\tvalidation-mlogloss:0.92088\n",
      "[82]\tvalidation-mlogloss:0.91846\n",
      "[83]\tvalidation-mlogloss:0.91703\n",
      "[84]\tvalidation-mlogloss:0.91507\n",
      "[85]\tvalidation-mlogloss:0.91302\n",
      "[86]\tvalidation-mlogloss:0.91110\n",
      "[87]\tvalidation-mlogloss:0.90933\n",
      "[88]\tvalidation-mlogloss:0.90754\n",
      "[89]\tvalidation-mlogloss:0.90586\n",
      "[90]\tvalidation-mlogloss:0.90337\n",
      "[91]\tvalidation-mlogloss:0.90119\n",
      "[92]\tvalidation-mlogloss:0.89988\n",
      "[93]\tvalidation-mlogloss:0.89825\n",
      "[94]\tvalidation-mlogloss:0.89673\n",
      "[95]\tvalidation-mlogloss:0.89526\n",
      "[96]\tvalidation-mlogloss:0.89351\n",
      "[97]\tvalidation-mlogloss:0.89153\n",
      "[98]\tvalidation-mlogloss:0.89013\n",
      "[99]\tvalidation-mlogloss:0.88824\n",
      "[100]\tvalidation-mlogloss:0.88636\n",
      "[101]\tvalidation-mlogloss:0.88454\n",
      "[102]\tvalidation-mlogloss:0.88284\n",
      "[103]\tvalidation-mlogloss:0.88105\n",
      "[104]\tvalidation-mlogloss:0.87913\n",
      "[105]\tvalidation-mlogloss:0.87742\n",
      "[106]\tvalidation-mlogloss:0.87544\n",
      "[107]\tvalidation-mlogloss:0.87429\n",
      "[108]\tvalidation-mlogloss:0.87272\n",
      "[109]\tvalidation-mlogloss:0.87100\n",
      "[110]\tvalidation-mlogloss:0.86942\n",
      "[111]\tvalidation-mlogloss:0.86756\n",
      "[112]\tvalidation-mlogloss:0.86579\n",
      "[113]\tvalidation-mlogloss:0.86439\n",
      "[114]\tvalidation-mlogloss:0.86339\n",
      "[115]\tvalidation-mlogloss:0.86199\n",
      "[116]\tvalidation-mlogloss:0.86066\n",
      "[117]\tvalidation-mlogloss:0.85952\n",
      "[118]\tvalidation-mlogloss:0.85813\n",
      "[119]\tvalidation-mlogloss:0.85680\n",
      "[120]\tvalidation-mlogloss:0.85566\n",
      "[121]\tvalidation-mlogloss:0.85421\n",
      "[122]\tvalidation-mlogloss:0.85240\n",
      "[123]\tvalidation-mlogloss:0.85106\n",
      "[124]\tvalidation-mlogloss:0.84956\n",
      "[125]\tvalidation-mlogloss:0.84795\n",
      "[126]\tvalidation-mlogloss:0.84670\n",
      "[127]\tvalidation-mlogloss:0.84539\n",
      "[128]\tvalidation-mlogloss:0.84348\n",
      "[129]\tvalidation-mlogloss:0.84193\n",
      "[130]\tvalidation-mlogloss:0.84079\n",
      "[131]\tvalidation-mlogloss:0.83912\n",
      "[132]\tvalidation-mlogloss:0.83743\n",
      "[133]\tvalidation-mlogloss:0.83600\n",
      "[134]\tvalidation-mlogloss:0.83498\n",
      "[135]\tvalidation-mlogloss:0.83357\n",
      "[136]\tvalidation-mlogloss:0.83258\n",
      "[137]\tvalidation-mlogloss:0.83165\n",
      "[138]\tvalidation-mlogloss:0.82994\n",
      "[139]\tvalidation-mlogloss:0.82841\n",
      "[140]\tvalidation-mlogloss:0.82745\n",
      "[141]\tvalidation-mlogloss:0.82594\n",
      "[142]\tvalidation-mlogloss:0.82465\n",
      "[143]\tvalidation-mlogloss:0.82335\n",
      "[144]\tvalidation-mlogloss:0.82209\n",
      "[145]\tvalidation-mlogloss:0.82117\n",
      "[146]\tvalidation-mlogloss:0.82023\n",
      "[147]\tvalidation-mlogloss:0.81909\n",
      "[148]\tvalidation-mlogloss:0.81724\n",
      "[149]\tvalidation-mlogloss:0.81620\n",
      "[150]\tvalidation-mlogloss:0.81479\n",
      "[151]\tvalidation-mlogloss:0.81347\n",
      "[152]\tvalidation-mlogloss:0.81222\n",
      "[153]\tvalidation-mlogloss:0.81096\n",
      "[154]\tvalidation-mlogloss:0.81001\n",
      "[155]\tvalidation-mlogloss:0.80835\n",
      "[156]\tvalidation-mlogloss:0.80715\n",
      "[157]\tvalidation-mlogloss:0.80609\n",
      "[158]\tvalidation-mlogloss:0.80497\n",
      "[159]\tvalidation-mlogloss:0.80370\n",
      "[160]\tvalidation-mlogloss:0.80242\n",
      "[161]\tvalidation-mlogloss:0.80117\n",
      "[162]\tvalidation-mlogloss:0.79980\n",
      "[163]\tvalidation-mlogloss:0.79849\n",
      "[164]\tvalidation-mlogloss:0.79692\n",
      "[165]\tvalidation-mlogloss:0.79555\n",
      "[166]\tvalidation-mlogloss:0.79399\n",
      "[167]\tvalidation-mlogloss:0.79305\n",
      "[168]\tvalidation-mlogloss:0.79200\n",
      "[169]\tvalidation-mlogloss:0.79102\n",
      "[170]\tvalidation-mlogloss:0.78997\n",
      "[171]\tvalidation-mlogloss:0.78904\n",
      "[172]\tvalidation-mlogloss:0.78776\n",
      "[173]\tvalidation-mlogloss:0.78690\n",
      "[174]\tvalidation-mlogloss:0.78578\n",
      "[175]\tvalidation-mlogloss:0.78484\n",
      "[176]\tvalidation-mlogloss:0.78346\n",
      "[177]\tvalidation-mlogloss:0.78219\n",
      "[178]\tvalidation-mlogloss:0.78135\n",
      "[179]\tvalidation-mlogloss:0.78046\n",
      "[180]\tvalidation-mlogloss:0.77947\n",
      "[181]\tvalidation-mlogloss:0.77830\n",
      "[182]\tvalidation-mlogloss:0.77716\n",
      "[183]\tvalidation-mlogloss:0.77617\n",
      "[184]\tvalidation-mlogloss:0.77497\n",
      "[185]\tvalidation-mlogloss:0.77407\n",
      "[186]\tvalidation-mlogloss:0.77317\n",
      "[187]\tvalidation-mlogloss:0.77232\n",
      "[188]\tvalidation-mlogloss:0.77117\n",
      "[189]\tvalidation-mlogloss:0.76997\n",
      "[190]\tvalidation-mlogloss:0.76907\n",
      "[191]\tvalidation-mlogloss:0.76818\n",
      "[192]\tvalidation-mlogloss:0.76738\n",
      "[193]\tvalidation-mlogloss:0.76646\n",
      "[194]\tvalidation-mlogloss:0.76519\n",
      "[195]\tvalidation-mlogloss:0.76396\n",
      "[196]\tvalidation-mlogloss:0.76258\n",
      "[197]\tvalidation-mlogloss:0.76153\n",
      "[198]\tvalidation-mlogloss:0.76042\n",
      "[199]\tvalidation-mlogloss:0.75910\n",
      "[200]\tvalidation-mlogloss:0.75792\n",
      "[201]\tvalidation-mlogloss:0.75694\n",
      "[202]\tvalidation-mlogloss:0.75616\n",
      "[203]\tvalidation-mlogloss:0.75516\n",
      "[204]\tvalidation-mlogloss:0.75391\n",
      "[205]\tvalidation-mlogloss:0.75299\n",
      "[206]\tvalidation-mlogloss:0.75210\n",
      "[207]\tvalidation-mlogloss:0.75147\n",
      "[208]\tvalidation-mlogloss:0.75010\n",
      "[209]\tvalidation-mlogloss:0.74918\n",
      "[210]\tvalidation-mlogloss:0.74859\n",
      "[211]\tvalidation-mlogloss:0.74762\n",
      "[212]\tvalidation-mlogloss:0.74677\n",
      "[213]\tvalidation-mlogloss:0.74610\n",
      "[214]\tvalidation-mlogloss:0.74514\n",
      "[215]\tvalidation-mlogloss:0.74455\n",
      "[216]\tvalidation-mlogloss:0.74377\n",
      "[217]\tvalidation-mlogloss:0.74320\n",
      "[218]\tvalidation-mlogloss:0.74202\n",
      "[219]\tvalidation-mlogloss:0.74090\n",
      "[220]\tvalidation-mlogloss:0.73991\n",
      "[221]\tvalidation-mlogloss:0.73905\n",
      "[222]\tvalidation-mlogloss:0.73805\n",
      "[223]\tvalidation-mlogloss:0.73718\n",
      "[224]\tvalidation-mlogloss:0.73654\n",
      "[225]\tvalidation-mlogloss:0.73568\n",
      "[226]\tvalidation-mlogloss:0.73452\n",
      "[227]\tvalidation-mlogloss:0.73341\n",
      "[228]\tvalidation-mlogloss:0.73274\n",
      "[229]\tvalidation-mlogloss:0.73193\n",
      "[230]\tvalidation-mlogloss:0.73114\n",
      "[231]\tvalidation-mlogloss:0.73007\n",
      "[232]\tvalidation-mlogloss:0.72946\n",
      "[233]\tvalidation-mlogloss:0.72893\n",
      "[234]\tvalidation-mlogloss:0.72780\n",
      "[235]\tvalidation-mlogloss:0.72702\n",
      "[236]\tvalidation-mlogloss:0.72639\n",
      "[237]\tvalidation-mlogloss:0.72549\n",
      "[238]\tvalidation-mlogloss:0.72495\n",
      "[239]\tvalidation-mlogloss:0.72437\n",
      "[240]\tvalidation-mlogloss:0.72338\n",
      "[241]\tvalidation-mlogloss:0.72262\n",
      "[242]\tvalidation-mlogloss:0.72194\n",
      "[243]\tvalidation-mlogloss:0.72117\n",
      "[244]\tvalidation-mlogloss:0.72054\n",
      "[245]\tvalidation-mlogloss:0.71976\n",
      "[246]\tvalidation-mlogloss:0.71923\n",
      "[247]\tvalidation-mlogloss:0.71823\n",
      "[248]\tvalidation-mlogloss:0.71745\n",
      "[249]\tvalidation-mlogloss:0.71634\n",
      "[250]\tvalidation-mlogloss:0.71594\n",
      "[251]\tvalidation-mlogloss:0.71510\n",
      "[252]\tvalidation-mlogloss:0.71385\n",
      "[253]\tvalidation-mlogloss:0.71330\n",
      "[254]\tvalidation-mlogloss:0.71230\n",
      "[255]\tvalidation-mlogloss:0.71131\n",
      "[256]\tvalidation-mlogloss:0.71071\n",
      "[257]\tvalidation-mlogloss:0.70995\n",
      "[258]\tvalidation-mlogloss:0.70938\n",
      "[259]\tvalidation-mlogloss:0.70901\n",
      "[260]\tvalidation-mlogloss:0.70833\n",
      "[261]\tvalidation-mlogloss:0.70768\n",
      "[262]\tvalidation-mlogloss:0.70716\n",
      "[263]\tvalidation-mlogloss:0.70648\n",
      "[264]\tvalidation-mlogloss:0.70601\n",
      "[265]\tvalidation-mlogloss:0.70532\n",
      "[266]\tvalidation-mlogloss:0.70451\n",
      "[267]\tvalidation-mlogloss:0.70393\n",
      "[268]\tvalidation-mlogloss:0.70281\n",
      "[269]\tvalidation-mlogloss:0.70224\n",
      "[270]\tvalidation-mlogloss:0.70126\n",
      "[271]\tvalidation-mlogloss:0.70038\n",
      "[272]\tvalidation-mlogloss:0.69981\n",
      "[273]\tvalidation-mlogloss:0.69943\n",
      "[274]\tvalidation-mlogloss:0.69834\n",
      "[275]\tvalidation-mlogloss:0.69780\n",
      "[276]\tvalidation-mlogloss:0.69696\n",
      "[277]\tvalidation-mlogloss:0.69619\n",
      "[278]\tvalidation-mlogloss:0.69542\n",
      "[279]\tvalidation-mlogloss:0.69451\n",
      "[280]\tvalidation-mlogloss:0.69371\n",
      "[281]\tvalidation-mlogloss:0.69304\n",
      "[282]\tvalidation-mlogloss:0.69247\n",
      "[283]\tvalidation-mlogloss:0.69182\n",
      "[284]\tvalidation-mlogloss:0.69083\n",
      "[285]\tvalidation-mlogloss:0.69027\n",
      "[286]\tvalidation-mlogloss:0.68981\n",
      "[287]\tvalidation-mlogloss:0.68922\n",
      "[288]\tvalidation-mlogloss:0.68817\n",
      "[289]\tvalidation-mlogloss:0.68719\n",
      "[290]\tvalidation-mlogloss:0.68642\n",
      "[291]\tvalidation-mlogloss:0.68568\n",
      "[292]\tvalidation-mlogloss:0.68490\n",
      "[293]\tvalidation-mlogloss:0.68406\n",
      "[294]\tvalidation-mlogloss:0.68332\n",
      "[295]\tvalidation-mlogloss:0.68278\n",
      "[296]\tvalidation-mlogloss:0.68230\n",
      "[297]\tvalidation-mlogloss:0.68189\n",
      "[298]\tvalidation-mlogloss:0.68114\n",
      "[299]\tvalidation-mlogloss:0.68059\n",
      "XGBoost Validation Accuracy: 83.50%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.93      0.83       266\n",
      "     Neutral       0.90      0.79      0.84       285\n",
      "    Positive       0.82      0.86      0.84       277\n",
      "  Irrelevant       0.95      0.72      0.81       172\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.86      0.82      0.83      1000\n",
      "weighted avg       0.85      0.83      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Convert training and validation sets to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=data['sentiment_numeric'])\n",
    "dval = xgb.DMatrix(X_val, label=validation_data['sentiment_numeric'])\n",
    "\n",
    "# Set up XGBoost parameters for CPU training\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Multi-class classification\n",
    "    'num_class': 4,                # Number of classes in the target variable\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'tree_method': 'hist',         # Use CPU for training\n",
    "    'eval_metric': 'mlogloss'      # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "bst = xgb.train(\n",
    "    params, \n",
    "    dtrain, \n",
    "    num_boost_round=300,           # Maximum number of boosting rounds\n",
    "    evals=[(dval, 'validation')],  # Validation set for evaluation\n",
    "    early_stopping_rounds=10,      # Stop if no improvement after 10 rounds\n",
    "    verbose_eval=True              # Print progress during training\n",
    ")\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred_xgb = bst.predict(dval)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(validation_data['sentiment_numeric'], y_pred_xgb)\n",
    "print(f'XGBoost Validation Accuracy: {accuracy_xgb * 100:.2f}%')\n",
    "print(classification_report(validation_data['sentiment_numeric'], y_pred_xgb, target_names=['Negative', 'Neutral', 'Positive', 'Irrelevant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
